{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Last modified: 10/05/2021\n",
    "\n",
    "@author: Daniel Rodriguez\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from keras.utils import to_categorical\n",
    "from Networks.Network import Network\n",
    "from Utilities.utils import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from ucimlrepo import fetch_ucirepo, list_available_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['uci_id', 'name', 'repository_url', 'data_url', 'abstract', 'area', 'tasks', 'characteristics', 'num_instances', 'num_features', 'feature_types', 'demographics', 'target_col', 'index_col', 'has_missing_values', 'missing_values_symbol', 'year_of_dataset_creation', 'last_updated', 'dataset_doi', 'creators', 'intro_paper', 'additional_info'])\n"
     ]
    }
   ],
   "source": [
    "wine = fetch_ucirepo(name='Wine')\n",
    "print(wine.metadata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            name     role         type demographic  \\\n",
      "0                          class   Target  Categorical        None   \n",
      "1                        Alcohol  Feature   Continuous        None   \n",
      "2                      Malicacid  Feature   Continuous        None   \n",
      "3                            Ash  Feature   Continuous        None   \n",
      "4              Alcalinity_of_ash  Feature   Continuous        None   \n",
      "5                      Magnesium  Feature      Integer        None   \n",
      "6                  Total_phenols  Feature   Continuous        None   \n",
      "7                     Flavanoids  Feature   Continuous        None   \n",
      "8           Nonflavanoid_phenols  Feature   Continuous        None   \n",
      "9                Proanthocyanins  Feature   Continuous        None   \n",
      "10               Color_intensity  Feature   Continuous        None   \n",
      "11                           Hue  Feature   Continuous        None   \n",
      "12  0D280_0D315_of_diluted_wines  Feature   Continuous        None   \n",
      "13                       Proline  Feature      Integer        None   \n",
      "\n",
      "   description units missing_values  \n",
      "0         None  None             no  \n",
      "1         None  None             no  \n",
      "2         None  None             no  \n",
      "3         None  None             no  \n",
      "4         None  None             no  \n",
      "5         None  None             no  \n",
      "6         None  None             no  \n",
      "7         None  None             no  \n",
      "8         None  None             no  \n",
      "9         None  None             no  \n",
      "10        None  None             no  \n",
      "11        None  None             no  \n",
      "12        None  None             no  \n",
      "13        None  None             no  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(wine.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape (rows,cols): (178, 13)\n",
      "Target Shape (rows,cols): (178,)\n"
     ]
    }
   ],
   "source": [
    "X, y = wine.data.features, wine.data.targets.squeeze().to_numpy()\n",
    "print(f\"Dataset Shape (rows,cols): {X.shape}\")\n",
    "print(f\"Target Shape (rows,cols): {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "RANDOM_STATE = 1\n",
    "RUN_EXPERIMENTS = True # if True, it will run the experiments\n",
    "BEST_MODEL = False # if True, it will run the best model\n",
    "EPOCHS = 400000 # just one since we are using a stopping criteria\n",
    "\n",
    "# Define hyperparameter grid for experiments\n",
    "params = {\n",
    "    'hidden_size': [5, 10, 20],\n",
    "    'learning_rate': [1.0, 0.1, 0.01, 0.001]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Number of samples in each dataset:\n",
      "Train shape: torch.Size([142, 13])\n",
      "Validation shape: torch.Size([8, 13])\n",
      "Test shape: torch.Size([28, 13])\n",
      "\n",
      "Shape of the target after one-hot encoding:\n",
      "Train shape: torch.Size([142, 3])\n",
      "Validation shape: torch.Size([8, 3])\n",
      "Test shape: torch.Size([28, 3])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\predictive_analytics\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Since all input features are numerical, we can use the StandardScaler to normalize the data\n",
    "numerical_preprocessing = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical', numerical_preprocessing, X.columns)\n",
    "])\n",
    "\n",
    "# Train, test, validation split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "# Preprocess the data and convert it to tensors\n",
    "X_train = torch.tensor(preprocessor.fit_transform(X_train), dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(preprocessor.transform(X_val), dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(preprocessor.transform(X_test), dtype=torch.float32).to(device)\n",
    "\n",
    "# One-hot encode the target\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_encoded = torch.tensor(encoder.fit_transform(y_train.reshape(-1, 1)), dtype=torch.long).to(device)\n",
    "y_val_encoded = torch.tensor(encoder.transform(y_val.reshape(-1, 1)), dtype=torch.long).to(device)\n",
    "y_test_encoded = torch.tensor(encoder.transform(y_test.reshape(-1, 1)), dtype=torch.long).to(device)\n",
    "\n",
    "# Data shapes\n",
    "print(\"Number of samples in each dataset:\")\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Validation shape: {X_val.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\"); print()\n",
    "\n",
    "# Target shapes\n",
    "print(\"Shape of the target after one-hot encoding:\")\n",
    "print(f\"Train shape: {y_train_encoded.shape}\")\n",
    "print(f\"Validation shape: {y_val_encoded.shape}\")\n",
    "print(f\"Test shape: {y_test_encoded.shape}\"); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Various Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING EXPERIMENTS\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer PE: 5\n",
      "Test loss pre training: 1.09132981300354\n",
      "\n",
      "Epoch: 0; before training loss: 1.177579402923584\n",
      "Epoch: 16961; after train loss: 0.00011563058069441468\n",
      "\n",
      "Test loss post training 0.004880609456449747\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer PE: 10\n",
      "Test loss pre training: 1.179295539855957\n",
      "\n",
      "Epoch: 0; before training loss: 1.0941189527511597\n",
      "Epoch: 17544; after train loss: 8.743775106268004e-05\n",
      "\n",
      "Test loss post training 0.005258933175355196\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer PE: 20\n",
      "Test loss pre training: 1.1005167961120605\n",
      "\n",
      "Epoch: 0; before training loss: 1.1118435859680176\n",
      "Epoch: 10; after train loss: 0.3592759668827057\n",
      "\n",
      "Test loss post training 0.316992849111557\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  2,  0],\n",
      "        [ 0, 53,  0],\n",
      "        [ 0,  3, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.1; Hidden Layer PE: 5\n",
      "Test loss pre training: 1.1929856538772583\n",
      "\n",
      "Epoch: 0; before training loss: 1.1473360061645508\n",
      "Epoch: 111625; after train loss: 0.00016973516903817654\n",
      "\n",
      "Test loss post training 0.0020836512558162212\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.1; Hidden Layer PE: 10\n",
      "Test loss pre training: 1.1924060583114624\n",
      "\n",
      "Epoch: 0; before training loss: 1.1793476343154907\n",
      "Epoch: 69226; after train loss: 0.00024003042199183255\n",
      "\n",
      "Test loss post training 0.003972179722040892\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.1; Hidden Layer PE: 20\n",
      "Test loss pre training: 1.1299173831939697\n",
      "\n",
      "Epoch: 0; before training loss: 1.1098746061325073\n",
      "Epoch: 5; after train loss: 1.062246322631836\n",
      "\n",
      "Test loss post training 1.0895336866378784\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[ 0,  3,  0],\n",
      "        [45, 55, 39],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[ 0,  1,  0],\n",
      "        [11,  7,  9],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.01; Hidden Layer PE: 5\n",
      "Test loss pre training: 1.3029404878616333\n",
      "\n",
      "Epoch: 0; before training loss: 1.1981403827667236\n",
      "Epoch: 114096; after train loss: 0.002793916966766119\n",
      "\n",
      "Test loss post training 0.006654083728790283\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.01; Hidden Layer PE: 10\n",
      "Test loss pre training: 1.1340820789337158\n",
      "\n",
      "Epoch: 0; before training loss: 1.1722468137741089\n",
      "Epoch: 163559; after train loss: 0.0011723615461960435\n",
      "\n",
      "Test loss post training 0.004328608978539705\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.01; Hidden Layer PE: 20\n",
      "Test loss pre training: 1.086639165878296\n",
      "\n",
      "Epoch: 0; before training loss: 1.1014801263809204\n",
      "Epoch: 173271; after train loss: 0.0011128544574603438\n",
      "\n",
      "Test loss post training 0.0034379693679511547\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.001; Hidden Layer PE: 5\n",
      "Test loss pre training: 1.158535122871399\n",
      "\n",
      "Epoch: 0; before training loss: 1.112593173980713\n",
      "Epoch: 5; after train loss: 1.112107276916504\n",
      "\n",
      "Test loss post training 1.1578733921051025\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[ 0,  0,  0],\n",
      "        [45, 58, 39],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[ 0,  0,  0],\n",
      "        [11,  8,  9],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.001; Hidden Layer PE: 10\n",
      "Test loss pre training: 1.1140568256378174\n",
      "\n",
      "Epoch: 0; before training loss: 1.1786831617355347\n",
      "Test loss post training 0.0061522359028458595\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.001; Hidden Layer PE: 20\n",
      "Test loss pre training: 1.0689622163772583\n",
      "\n",
      "Epoch: 0; before training loss: 1.126001000404358\n",
      "Epoch: 375397; after train loss: 0.0066636111587285995\n",
      "\n",
      "Test loss post training 0.0060800062492489815\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "DONE TESTING HYPERPARAMETERS\n"
     ]
    }
   ],
   "source": [
    "if RUN_EXPERIMENTS:\n",
    "    print(\"RUNNING EXPERIMENTS\")\n",
    "    #output storage lists\n",
    "    posttrain_loss_storage = torch.zeros(len(params['learning_rate']), len(params['hidden_size']), dtype = torch.float32)\n",
    "    train_confusion_matrix_storage = np.empty((4,3,3,3), dtype = np.int_)\n",
    "    test_confusion_matrix_storage = np.empty((4,3,3,3), dtype = np.int_)\n",
    "    \n",
    "    lr_count = 0\n",
    "    hs_cound = 0\n",
    "    for i in params['learning_rate']:\n",
    "        hs_count = 0\n",
    "        for j in params['hidden_size']:\n",
    "            num_hidden = j\n",
    "            model = Network(num_hidden)\n",
    "            stopping_criteria = StopCriteria()\n",
    "\n",
    "            #sending model to cuda\n",
    "            model.to(device)\n",
    "            #X_train.to(device)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss() #cross-entropy loss\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr = i) # implementing momentum for learning rate\n",
    "\n",
    "            #Showing test set loss pre-training\n",
    "            print(\"-----------------------------------------------------------------\")\n",
    "            print(\"Learning Rate: \" + str(i) + \"; \" + \"Hidden Layer PE: \" + str(j))\n",
    "            \n",
    "            # Training model\n",
    "            for epoch in range(EPOCHS):\n",
    "                optimizer.zero_grad()\n",
    "                output = model.forward(X_train)\n",
    "                loss = criterion(output.squeeze(), torch.max(y_train_encoded, 1)[1]) \n",
    "                \n",
    "                if epoch == 0:\n",
    "                    print('Epoch: {}; before training loss: {}'.format(epoch,loss.item()))\n",
    "\n",
    "                #implementing stopping criteria\n",
    "                val_output = model.forward(X_val)\n",
    "                val_loss = criterion(val_output.squeeze(), torch.max(y_val_encoded, 1)[1])\n",
    "\n",
    "                if stopping_criteria.step(val_loss):\n",
    "                    print('Epoch: {}; after train loss: {}'.format(epoch,loss.item()))\n",
    "                    print()\n",
    "                    break\n",
    "\n",
    "                # #printing epoch and loss \n",
    "                # if epoch % 5000 == 0:\n",
    "                #     print('Epoch: {} train loss: {}'.format(epoch,loss.item()))\n",
    "\n",
    "                #backpropagation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            # evaluating the model and storing relevant information\n",
    "            posttrain_loss_storage[lr_count, hs_count] = loss\n",
    "            \n",
    "            model.eval()\n",
    "            train_pred = model(X_train)\n",
    "            test_pred = model(X_test)\n",
    "            train_CM = ConfusionMatrix(model, train_pred, y_train_encoded)\n",
    "            test_CM = ConfusionMatrix(model, test_pred, y_test_encoded)\n",
    "            after_train = criterion(test_pred.squeeze(), torch.max(y_test_encoded, 1)[1])\n",
    "            \n",
    "            print('Test loss post training' , after_train.item())\n",
    "            print()\n",
    "            print(\"Training Confusion Matrix: \\n\" + str(train_CM))\n",
    "            print()\n",
    "            print(\"Test Confusion Matrix: \\n\" + str(test_CM))\n",
    "            print()\n",
    "            print(\"-----------------------------------------------------------------\")\n",
    "            \n",
    "            #train_confusion_matrix_storage[lr_count,hs_count,:,:] = train_CM\n",
    "            #test_confusion_matrix_storage[lr_count,hs_count,:,:] = test_CM\n",
    "            \n",
    "            hs_count += 1\n",
    "        lr_count += 1\n",
    "    \n",
    "    print()\n",
    "    print(\"DONE TESTING HYPERPARAMETERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr_index, best_hs_index = torch.argmin(posttrain_loss_storage).tolist()\n",
    "best_lr = params['learning_rate'][best_lr_index]\n",
    "best_hs = params['hidden_size'][best_hs_index]\n",
    "\n",
    "print(\"Best Learning Rate: \" + str(best_lr))    \n",
    "print(\"Best Hidden Layer PE: \" + str(best_hs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BEST_MODEL:\n",
    "    print(\"RUNNING BEST MODEL\")\n",
    "\n",
    "    #Initializing model and stopping criteria classes\n",
    "    model = Network(best_hs)\n",
    "    stopping_criteria = StopCriteria()\n",
    "\n",
    "    #sending model to cuda\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss() #cross-entropy loss\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = best_lr) # implementing momentum for learning rate\n",
    "\n",
    "    # Training model\n",
    "    for epoch in range(EPOCHS):\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(X_train)\n",
    "        loss = criterion(output.squeeze(), torch.max(y_train_encoded, 1)[1]) \n",
    "\n",
    "        #implementing stopping criteria\n",
    "        val_output = model.forward(X_val)\n",
    "        val_loss = criterion(val_output.squeeze(), torch.max(y_val_encoded, 1)[1])\n",
    "\n",
    "        if stopping_criteria.step(val_loss):\n",
    "            print('Epoch: {}; after train loss: {}'.format(epoch,loss.item()))\n",
    "            break\n",
    "\n",
    "        #printing epoch and loss \n",
    "        if epoch % 250 == 0:\n",
    "            print('Epoch: {}; train loss: {}'.format(epoch,loss.item()))\n",
    "\n",
    "        #backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after Training 0.0060800062492489815\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(X_test)\n",
    "after_train = criterion(y_pred.squeeze(), torch.max(y_test_encoded, 1)[1]) \n",
    "print('Test loss after Training' , after_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Confusion Matrix:\n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n"
     ]
    }
   ],
   "source": [
    "test_CM = ConfusionMatrix(model, y_pred, y_test_encoded)\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(test_CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Model Discussion:\n",
    "\n",
    "The overall best model in terms of test set loss after training is the model with a learning rate of 1 and 5 hidden PE. This model achieved a testing loss of 0.00117. This learning rate may be performing the best because it can quickly get over non-optimal local minima on the performance surface before the training is terminated by the stopping criteria. It may be possible to achieve lower train and test loss with more patience on the stopping criteria for lower learning rates - however this would increase the number of epochs required for convergence and increase computation time. Five PE in the hidden layer may be performing well because not all of the inputs are predictive of the wine cultivar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network2(nn.Module):\n",
    "    def __init__(self, num_hidden1, num_hidden2):\n",
    "        super(Network2, self).__init__()\n",
    "        self.num_hidden1 = num_hidden1\n",
    "        self.num_hidden2 = num_hidden2\n",
    "        \n",
    "        # Inputs to hidden linear combination\n",
    "        self.hidden1 = nn.Linear(13, self.num_hidden1)\n",
    "        # hidden to output layer, 3 classes - one for each cultivar\n",
    "        self.hidden2 = nn.Linear(self.num_hidden1, self.num_hidden2)\n",
    "        self.out = nn.Linear(self.num_hidden2, 3)\n",
    "        \n",
    "        # Defining activation functions\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = self.hidden1(x)\n",
    "        out1 = self.sigmoid(z1)\n",
    "        z2 = self.hidden2(out1)\n",
    "        out2 = self.sigmoid(z2)\n",
    "        z3 = self.out(out2)\n",
    "        \n",
    "        return z3\n",
    "    \n",
    "    def prediction(self, output):\n",
    "        preds = torch.zeros(1,output.shape[0]).flatten().long()\n",
    "        for i in range(len(preds)):\n",
    "            index = torch.argmax(output[i,:])\n",
    "            \n",
    "            if index == 0:\n",
    "                preds[i] = 1\n",
    "            elif index == 1:\n",
    "                preds[i] = 2\n",
    "            else:\n",
    "                preds[i] = 3\n",
    "                \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer 1 PE: 5; Hidden Layer 2 PE: 5\n",
      "Test loss pre training: 1.2559295892715454\n",
      "\n",
      "Epoch: 0; before training loss: 1.187574863433838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22661; after train loss: 6.319114618236199e-05\n",
      "\n",
      "Test loss post training 0.0002485540753696114\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer 1 PE: 5; Hidden Layer 2 PE: 10\n",
      "Test loss pre training: 1.1136878728866577\n",
      "\n",
      "Epoch: 0; before training loss: 1.090362310409546\n",
      "Epoch: 6; after train loss: 1.084182620048523\n",
      "\n",
      "Test loss post training 1.1220940351486206\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[ 0,  0,  0],\n",
      "        [45, 58, 39],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[ 0,  0,  0],\n",
      "        [11,  8,  9],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer 1 PE: 10; Hidden Layer 2 PE: 5\n",
      "Test loss pre training: 1.2219537496566772\n",
      "\n",
      "Epoch: 0; before training loss: 1.2208458185195923\n",
      "Epoch: 18710; after train loss: 7.625830039614812e-05\n",
      "\n",
      "Test loss post training 0.000352130678948015\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer 1 PE: 10; Hidden Layer 2 PE: 10\n",
      "Test loss pre training: 1.1350016593933105\n",
      "\n",
      "Epoch: 0; before training loss: 1.1277248859405518\n",
      "Epoch: 6; after train loss: 1.0769851207733154\n",
      "\n",
      "Test loss post training 1.1132961511611938\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[ 0,  0,  0],\n",
      "        [45, 58, 39],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[ 0,  0,  0],\n",
      "        [11,  8,  9],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.1; Hidden Layer 1 PE: 5; Hidden Layer 2 PE: 5\n",
      "Test loss pre training: 1.1117268800735474\n",
      "\n",
      "Epoch: 0; before training loss: 1.1927229166030884\n",
      "Epoch: 109866; after train loss: 0.00019061096827499568\n",
      "\n",
      "Test loss post training 0.0008215081761591136\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.1; Hidden Layer 1 PE: 5; Hidden Layer 2 PE: 10\n",
      "Test loss pre training: 1.1008270978927612\n",
      "\n",
      "Epoch: 0; before training loss: 1.120227575302124\n",
      "Epoch: 37941; after train loss: 0.0003300358366686851\n",
      "\n",
      "Test loss post training 0.0012553053675219417\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.1; Hidden Layer 1 PE: 10; Hidden Layer 2 PE: 5\n",
      "Test loss pre training: 1.219381332397461\n",
      "\n",
      "Epoch: 0; before training loss: 1.2120660543441772\n",
      "Epoch: 79662; after train loss: 0.00020912241598125547\n",
      "\n",
      "Test loss post training 0.0004526438133325428\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.1; Hidden Layer 1 PE: 10; Hidden Layer 2 PE: 10\n",
      "Test loss pre training: 1.1297005414962769\n",
      "\n",
      "Epoch: 0; before training loss: 1.1380101442337036\n",
      "Epoch: 88073; after train loss: 0.0001164921122835949\n",
      "\n",
      "Test loss post training 0.0005450330208986998\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "DONE TESTING HYPERPARAMETERS\n"
     ]
    }
   ],
   "source": [
    "if RUN_EXPERIMENTS:\n",
    "    #Hyper-Parameter sets\n",
    "    learning_rates = np.array([1.0, 0.1])\n",
    "    hidden_sizes1 = np.array([5, 10])\n",
    "    hidden_sizes2 = np.array([5, 10])\n",
    "    EPOCHS = 400000 # just one since we are using a stopping criteria\n",
    "    \n",
    "    #output storage lists\n",
    "    \"\"\"posttrain_loss_storage2 = torch.zeros(len(learning_rates), len(hidden_sizes), dtype = torch.float32)\n",
    "    train_confusion_matrix_storage = np.empty((4,3,3,3), dtype = np.int_)\n",
    "    test_confusion_matrix_storage = np.empty((4,3,3,3), dtype = np.int_)\"\"\"\n",
    "    \n",
    "    lr_count = 0\n",
    "    for i in learning_rates:\n",
    "        hs_count1 = 0\n",
    "        for j in hidden_sizes1:\n",
    "            num_hidden1 = j\n",
    "            hs_count2 = 0\n",
    "            for k in hidden_sizes2:\n",
    "                num_hidden2 = k\n",
    "                \n",
    "                model2 = Network2(num_hidden1, num_hidden2)\n",
    "                stopping_criteria = StopCriteria()\n",
    "\n",
    "                #sending model to cuda\n",
    "                device = torch.device(\"cuda:0\")\n",
    "                model2.to(device)\n",
    "                #X_train.to(device)\n",
    "\n",
    "                criterion = nn.CrossEntropyLoss() #cross-entropy loss\n",
    "                optimizer = torch.optim.SGD(model2.parameters(), lr = i) # implementing momentum for learning rate\n",
    "\n",
    "                #Showing test set loss pre-training\n",
    "                print(\"-----------------------------------------------------------------\")\n",
    "                print(\"Learning Rate: \" + str(i) + \"; \" + \"Hidden Layer 1 PE: \" + str(j) + \"; \" + \"Hidden Layer 2 PE: \" + str(k))\n",
    "\n",
    "                model2.eval()\n",
    "                y_pred = model2(X_test)\n",
    "                before_train = criterion(y_pred.squeeze(), torch.max(y_test_encoded, 1)[1])\n",
    "                print(\"Test loss pre training: \" + str(before_train.item()))\n",
    "                print()\n",
    "\n",
    "                # Training model\n",
    "                for epoch in range(EPOCHS):\n",
    "                    optimizer.zero_grad()\n",
    "                    output = model2.forward(X_train)\n",
    "                    loss = criterion(output.squeeze(), torch.max(y_train_encoded, 1)[1]) \n",
    "\n",
    "                    if epoch == 0:\n",
    "                        print('Epoch: {}; before training loss: {}'.format(epoch,loss.item()))\n",
    "\n",
    "                    #implementing stopping criteria\n",
    "                    val_output = model2.forward(X_val)\n",
    "                    val_loss = criterion(val_output.squeeze(), torch.max(y_val_encoded, 1)[1])\n",
    "\n",
    "                    if stopping_criteria.step(val_loss):\n",
    "                        print('Epoch: {}; after train loss: {}'.format(epoch,loss.item()))\n",
    "                        print()\n",
    "                        break\n",
    "\n",
    "                    \"\"\"\n",
    "                    #printing epoch and loss \n",
    "                    if epoch % 5000 == 0:\n",
    "                        print('Epoch: {} train loss: {}'.format(epoch,loss.item()))\n",
    "                    \"\"\"\n",
    "\n",
    "                    #backpropagation\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # evaluating the model and storing relevant information\n",
    "                #posttrain_loss_storage2[lr_count, hs_count] = loss\n",
    "\n",
    "                model2.eval()\n",
    "                train_pred = model2(X_train)\n",
    "                test_pred = model2(X_test)\n",
    "                train_CM = ConfusionMatrix(model2, train_pred, y_train_encoded)\n",
    "                test_CM = ConfusionMatrix(model2, test_pred, y_test_encoded)\n",
    "                after_train = criterion(test_pred.squeeze(), torch.max(y_test_encoded, 1)[1])\n",
    "\n",
    "                print('Test loss post training' , after_train.item())\n",
    "                print()\n",
    "                print(\"Training Confusion Matrix: \\n\" + str(train_CM))\n",
    "                print()\n",
    "                print(\"Test Confusion Matrix: \\n\" + str(test_CM))\n",
    "                print()\n",
    "                print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "                #train_confusion_matrix_storage[lr_count,hs_count,:,:] = train_CM\n",
    "                #test_confusion_matrix_storage[lr_count,hs_count,:,:] = test_CM\n",
    "                hs_count2 += 1\n",
    "            hs_count1 += 1\n",
    "        lr_count += 1\n",
    "    \n",
    "    print()\n",
    "    print(\"DONE TESTING HYPERPARAMETERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of Two Hidden Layer Models\n",
    "\n",
    "Taking the best learning rates and layer size from the previous model and testing all combinations I found that introducing an additional layer to the model does not significantly increase model performance for classification but it does occasionaly perform better on test set loss when controlling for the train-validation-test split of the data. Both models are able to achieve perfect classification, when the stopping criteria isnt met early, in less than 5K epochs - with the 1 hidden layer model taking less epochs on average. \n",
    "\n",
    "Some combinations of hyper-parameters caused the model training to terminate early. This is likely due to the stop-criteria not giving the model enough time to get out of a local minima before the stop criteria was met. Increasing the stop-criteria to 10 does help reduce the number of models terminating early but some still terminate early."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model with no hidden-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network3, self).__init__()\n",
    "        \n",
    "        # Inputs to outputs linear combination\n",
    "        self.out = nn.Linear(13, 3)\n",
    "        \n",
    "        # Defining activation functions\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z1 = self.out(x)\n",
    "        \n",
    "        return z1\n",
    "    \n",
    "    def prediction(self, output):\n",
    "        preds = torch.zeros(1,output.shape[0]).flatten().long()\n",
    "        for i in range(len(preds)):\n",
    "            index = torch.argmax(output[i,:])\n",
    "            \n",
    "            if index == 0:\n",
    "                preds[i] = 1\n",
    "            elif index == 1:\n",
    "                preds[i] = 2\n",
    "            else:\n",
    "                preds[i] = 3\n",
    "                \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0\n",
      "Test loss pre training: 1.6084026098251343\n",
      "\n",
      "Epoch: 0; before training loss: 1.4049665927886963\n",
      "Epoch: 0 train loss: 1.4049665927886963\n",
      "Epoch: 2 train loss: 1.4049665927886963\n",
      "Epoch: 4 train loss: 1.4049665927886963\n",
      "Epoch: 6 train loss: 1.4049665927886963\n",
      "Epoch: 8 train loss: 1.4049665927886963\n",
      "Epoch: 10; after train loss: 1.4049665927886963\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CrossEntropyLoss.forward() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m train_CM \u001b[38;5;241m=\u001b[39m ConfusionMatrix(model3, train_pred, y_train_encoded)\n\u001b[0;32m     62\u001b[0m test_CM \u001b[38;5;241m=\u001b[39m ConfusionMatrix(model3, test_pred, y_test_encoded)\n\u001b[1;32m---> 63\u001b[0m after_train \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest loss post training\u001b[39m\u001b[38;5;124m'\u001b[39m , after_train\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\predictive_analytics\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\predictive_analytics\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: CrossEntropyLoss.forward() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "if RUN_EXPERIMENTS:\n",
    "    #Hyper-Parameter sets\n",
    "    learning_rates = np.array([1.0, 0.1, 0.01, 0.001])\n",
    "    EPOCHS = 400000 # just one since we are using a stopping criteria\n",
    "    \n",
    "    lr_count = 0\n",
    "    for i in learning_rates:\n",
    "        model3 = Network3()\n",
    "        stopping_criteria = StopCriteria(patience = 10)\n",
    "\n",
    "        #sending model to cuda\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        model3.to(device)\n",
    "        #X_train.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss() #cross-entropy loss\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = i) # implementing momentum for learning rate\n",
    "\n",
    "        #Showing test set loss pre-training\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Learning Rate: \" + str(i))\n",
    "\n",
    "        model3.eval()\n",
    "        y_pred = model3(X_test)\n",
    "        before_train = criterion(y_pred.squeeze(), torch.max(y_test_encoded, 1)[1])\n",
    "        print(\"Test loss pre training: \" + str(before_train.item()))\n",
    "        print()\n",
    "\n",
    "        # Training model\n",
    "        for epoch in range(EPOCHS):\n",
    "            optimizer.zero_grad()\n",
    "            output = model3.forward(X_train)\n",
    "            loss = criterion(output.squeeze(), torch.max(y_train_encoded, 1)[1]) \n",
    "\n",
    "            if epoch == 0:\n",
    "                print('Epoch: {}; before training loss: {}'.format(epoch,loss.item()))\n",
    "\n",
    "            #implementing stopping criteria\n",
    "            val_output = model3.forward(X_val)\n",
    "            val_loss = criterion(val_output.squeeze(), torch.max(y_val_encoded, 1)[1])\n",
    "\n",
    "            if stopping_criteria.step(val_loss):\n",
    "                print('Epoch: {}; after train loss: {}'.format(epoch,loss.item()))\n",
    "                print()\n",
    "                break\n",
    "\n",
    "            \n",
    "            #printing epoch and loss \n",
    "            if epoch % 2 == 0:\n",
    "                print('Epoch: {} train loss: {}'.format(epoch,loss.item()))\n",
    "            \n",
    "\n",
    "            #backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # evaluating the model and storing relevant information\n",
    "        model3.eval()\n",
    "        train_pred = model3(X_train)\n",
    "        test_pred = model3(X_test)\n",
    "        train_CM = ConfusionMatrix(model3, train_pred, y_train_encoded)\n",
    "        test_CM = ConfusionMatrix(model3, test_pred, y_test_encoded)\n",
    "        after_train = criterion(test_pred.squeeze(), torch.max(y_test_encoded, 1)[1])\n",
    "\n",
    "        print('Test loss post training' , after_train.item())\n",
    "        print()\n",
    "        print(\"Training Confusion Matrix: \\n\" + str(train_CM))\n",
    "        print()\n",
    "        print(\"Test Confusion Matrix: \\n\" + str(test_CM))\n",
    "        print()\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "        #train_confusion_matrix_storage[lr_count,hs_count,:,:] = train_CM\n",
    "        #test_confusion_matrix_storage[lr_count,hs_count,:,:] = test_CM\n",
    "\n",
    "    lr_count += 1\n",
    "    \n",
    "print()\n",
    "print(\"DONE TESTING HYPERPARAMETERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of no hidden layer model:\n",
    "\n",
    "A model with no hidden layers is unable to classify the Wine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
