{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Last modified: 10/05/2021\n",
    "\n",
    "@author: Daniel Rodriguez\n",
    "\"\"\"\n",
    "import itertools\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Networks.Network import Network\n",
    "from Utilities.utils import *\n",
    "from Utilities.run_experiments import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from ucimlrepo import fetch_ucirepo, list_available_datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['uci_id', 'name', 'repository_url', 'data_url', 'abstract', 'area', 'tasks', 'characteristics', 'num_instances', 'num_features', 'feature_types', 'demographics', 'target_col', 'index_col', 'has_missing_values', 'missing_values_symbol', 'year_of_dataset_creation', 'last_updated', 'dataset_doi', 'creators', 'intro_paper', 'additional_info'])\n"
     ]
    }
   ],
   "source": [
    "wine = fetch_ucirepo(name='Wine')\n",
    "print(wine.metadata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            name     role         type demographic  \\\n",
      "0                          class   Target  Categorical        None   \n",
      "1                        Alcohol  Feature   Continuous        None   \n",
      "2                      Malicacid  Feature   Continuous        None   \n",
      "3                            Ash  Feature   Continuous        None   \n",
      "4              Alcalinity_of_ash  Feature   Continuous        None   \n",
      "5                      Magnesium  Feature      Integer        None   \n",
      "6                  Total_phenols  Feature   Continuous        None   \n",
      "7                     Flavanoids  Feature   Continuous        None   \n",
      "8           Nonflavanoid_phenols  Feature   Continuous        None   \n",
      "9                Proanthocyanins  Feature   Continuous        None   \n",
      "10               Color_intensity  Feature   Continuous        None   \n",
      "11                           Hue  Feature   Continuous        None   \n",
      "12  0D280_0D315_of_diluted_wines  Feature   Continuous        None   \n",
      "13                       Proline  Feature      Integer        None   \n",
      "\n",
      "   description units missing_values  \n",
      "0         None  None             no  \n",
      "1         None  None             no  \n",
      "2         None  None             no  \n",
      "3         None  None             no  \n",
      "4         None  None             no  \n",
      "5         None  None             no  \n",
      "6         None  None             no  \n",
      "7         None  None             no  \n",
      "8         None  None             no  \n",
      "9         None  None             no  \n",
      "10        None  None             no  \n",
      "11        None  None             no  \n",
      "12        None  None             no  \n",
      "13        None  None             no  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(wine.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape (rows,cols): (178, 13)\n",
      "Target Shape (rows,cols): (178,)\n"
     ]
    }
   ],
   "source": [
    "X, y = wine.data.features, wine.data.targets.squeeze().to_numpy()\n",
    "print(f\"Dataset Shape (rows,cols): {X.shape}\")\n",
    "print(f\"Target Shape (rows,cols): {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Number of samples in each dataset:\n",
      "Train shape: torch.Size([142, 13])\n",
      "Validation shape: torch.Size([8, 13])\n",
      "Test shape: torch.Size([28, 13])\n",
      "\n",
      "Shape of the target after one-hot encoding:\n",
      "Train shape: torch.Size([142, 3])\n",
      "Validation shape: torch.Size([8, 3])\n",
      "Test shape: torch.Size([28, 3])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\envs\\act_smile\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 1\n",
    "\n",
    "# Since all input features are numerical, we can use the StandardScaler to normalize the data\n",
    "numerical_preprocessing = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical', numerical_preprocessing, X.columns)\n",
    "])\n",
    "\n",
    "# Train, test, validation split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "# Device configuration\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "# Preprocess the data and convert it to tensors\n",
    "X_train = torch.tensor(preprocessor.fit_transform(X_train), dtype=torch.float32).to(device)\n",
    "X_val = torch.tensor(preprocessor.transform(X_val), dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(preprocessor.transform(X_test), dtype=torch.float32).to(device)\n",
    "\n",
    "# One-hot encode the target\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_encoded = torch.tensor(encoder.fit_transform(y_train.reshape(-1, 1)), dtype=torch.long).to(device)\n",
    "y_val_encoded = torch.tensor(encoder.transform(y_val.reshape(-1, 1)), dtype=torch.long).to(device)\n",
    "y_test_encoded = torch.tensor(encoder.transform(y_test.reshape(-1, 1)), dtype=torch.long).to(device)\n",
    "\n",
    "# Data shapes\n",
    "print(\"Number of samples in each dataset:\")\n",
    "print(f\"Train shape: {X_train.shape}\")\n",
    "print(f\"Validation shape: {X_val.shape}\")\n",
    "print(f\"Test shape: {X_test.shape}\"); print()\n",
    "\n",
    "# Target shapes\n",
    "print(\"Shape of the target after one-hot encoding:\")\n",
    "print(f\"Train shape: {y_train_encoded.shape}\")\n",
    "print(f\"Validation shape: {y_val_encoded.shape}\")\n",
    "print(f\"Test shape: {y_test_encoded.shape}\"); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network with One Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Various Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL PARAMETERS\n",
    "RUN_EXPERIMENTS = True # if True, it will run the experiments\n",
    "BEST_MODEL = False # if True, it will run the best model\n",
    "EPOCHS = 400000\n",
    "\n",
    "# LOCAL PARAMETERS\n",
    "NUM_LAYERS = 1\n",
    "NUM_INPUTS = X_train.shape[1]\n",
    "NUM_OUTPUTS = y_train_encoded.shape[1]\n",
    "\n",
    "# Define hyperparameter grid for experiments\n",
    "params = {\n",
    "    'hidden_size': [[5], [10], [20]],\n",
    "    'learning_rate': [1.0, 0.1, 0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Experiments...\n",
      "---------------------------------------------\n",
      "Hidden Layer Shape: [5]; Learning Rate: 1.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training loss: 1.0928658246994019\n",
      "Training stopped at epoch 52343 after 59.06 seconds\n",
      "\n",
      "Post-training Results:\n",
      "Train Loss: 0.0000; Validation Loss: 0.0041; Test Loss: 0.0018\n",
      "Training Accuracy: 1.0000; Test Accuracy: 1.0000\n",
      "\n",
      "Train Confusion Matrix:\n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix:\n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Hidden Layer Shape: [10]; Learning Rate: 1.0\n",
      "\n",
      "Initial training loss: 1.1048319339752197\n",
      "Training stopped at epoch 63806 after 78.38 seconds\n",
      "\n",
      "Post-training Results:\n",
      "Train Loss: 0.0000; Validation Loss: 0.0026; Test Loss: 0.0053\n",
      "Training Accuracy: 1.0000; Test Accuracy: 1.0000\n",
      "\n",
      "Train Confusion Matrix:\n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix:\n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Hidden Layer Shape: [20]; Learning Rate: 1.0\n",
      "\n",
      "Initial training loss: 1.163552165031433\n",
      "Training stopped at epoch 21917 after 25.60 seconds\n",
      "\n",
      "Post-training Results:\n",
      "Train Loss: 0.0001; Validation Loss: 0.0005; Test Loss: 0.0077\n",
      "Training Accuracy: 1.0000; Test Accuracy: 1.0000\n",
      "\n",
      "Train Confusion Matrix:\n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix:\n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Hidden Layer Shape: [5]; Learning Rate: 0.1\n",
      "\n",
      "Initial training loss: 1.176382064819336\n",
      "Training stopped at epoch 40509 after 44.81 seconds\n",
      "\n",
      "Post-training Results:\n",
      "Train Loss: 0.0005; Validation Loss: 0.0009; Test Loss: 0.0032\n",
      "Training Accuracy: 1.0000; Test Accuracy: 1.0000\n",
      "\n",
      "Train Confusion Matrix:\n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix:\n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Hidden Layer Shape: [10]; Learning Rate: 0.1\n",
      "\n",
      "Initial training loss: 1.1164593696594238\n",
      "Training stopped at epoch 11 after 0.01 seconds\n",
      "\n",
      "Post-training Results:\n",
      "Train Loss: 1.0549; Validation Loss: 0.9265; Test Loss: 1.0896\n",
      "Training Accuracy: 0.4085; Test Accuracy: 0.2857\n",
      "\n",
      "Train Confusion Matrix:\n",
      "tensor([[ 0,  0,  0],\n",
      "        [45, 58, 39],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "Test Confusion Matrix:\n",
      "tensor([[ 0,  0,  1],\n",
      "        [11,  8,  8],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Hidden Layer Shape: [20]; Learning Rate: 0.1\n",
      "\n",
      "Initial training loss: 1.1467704772949219\n",
      "Training stopped at epoch 103815 after 116.22 seconds\n",
      "\n",
      "Post-training Results:\n",
      "Train Loss: 0.0001; Validation Loss: 0.0045; Test Loss: 0.0054\n",
      "Training Accuracy: 1.0000; Test Accuracy: 1.0000\n",
      "\n",
      "Train Confusion Matrix:\n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix:\n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Hidden Layer Shape: [5]; Learning Rate: 0.01\n",
      "\n",
      "Initial training loss: 1.2366812229156494\n",
      "Training stopped at epoch 211369 after 235.01 seconds\n",
      "\n",
      "Post-training Results:\n",
      "Train Loss: 0.0011; Validation Loss: 0.0094; Test Loss: 0.0049\n",
      "Training Accuracy: 1.0000; Test Accuracy: 1.0000\n",
      "\n",
      "Train Confusion Matrix:\n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix:\n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Hidden Layer Shape: [10]; Learning Rate: 0.01\n",
      "\n",
      "Initial training loss: 1.2462290525436401\n",
      "Training stopped at epoch 157288 after 175.19 seconds\n",
      "\n",
      "Post-training Results:\n",
      "Train Loss: 0.0013; Validation Loss: 0.0046; Test Loss: 0.0048\n",
      "Training Accuracy: 1.0000; Test Accuracy: 1.0000\n",
      "\n",
      "Train Confusion Matrix:\n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix:\n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Hidden Layer Shape: [20]; Learning Rate: 0.01\n",
      "\n",
      "Initial training loss: 1.1321078538894653\n",
      "Training stopped at epoch 138246 after 156.10 seconds\n",
      "\n",
      "Post-training Results:\n",
      "Train Loss: 0.0015; Validation Loss: 0.0040; Test Loss: 0.0030\n",
      "Training Accuracy: 1.0000; Test Accuracy: 1.0000\n",
      "\n",
      "Train Confusion Matrix:\n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix:\n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "---------------------------------------------\n",
      "Experiments Completed\n"
     ]
    }
   ],
   "source": [
    "if RUN_EXPERIMENTS:\n",
    "    print(\"Running Experiments...\")\n",
    "\n",
    "    training_losses, validation_losses, training_accuracies, validation_accuracies = [], [], [], []\n",
    "    test_losses, test_accuracies = [], []\n",
    "    for hs in params['hidden_size']:\n",
    "        sub_training_losses, sub_validation_losses, sub_training_accuracies, sub_validation_accuracies = [], [], [], []\n",
    "        sub_test_loss, sub_test_accuracy = [], []\n",
    "        for lr in params['learning_rate']:\n",
    "\n",
    "            # Create the model\n",
    "            model = Network(NUM_INPUTS, NUM_OUTPUTS, NUM_LAYERS, hs).to(device)\n",
    "\n",
    "            print(\"---------------------------------------------\")\n",
    "            print(f\"Hidden Layer Shape: {hs}; Learning Rate: {lr}\\n\")\n",
    "\n",
    "            # Train the model\n",
    "            model, train_loss, val_loss, train_cm, val_cm, epoch_stop, time = train_model(model, EPOCHS, lr, X_train, y_train_encoded, X_val, y_val_encoded, verbose=True)\n",
    "            training_accuracy = (train_cm[0,0] + train_cm[1,1] + train_cm[2,2]) / torch.sum(train_cm)\n",
    "            validation_accuracy = (val_cm[0,0] + val_cm[1,1] + val_cm[2,2]) / torch.sum(val_cm)\n",
    "            \n",
    "            print(f\"Training stopped at epoch {epoch_stop} after {time:.2f} seconds\\n\")\n",
    "\n",
    "            # Evaluate the model on the test set\n",
    "            test_loss, test_cm = evaluate_model(model, X_test, y_test_encoded)\n",
    "            test_accuracy = (test_cm[0,0] + test_cm[1,1] + test_cm[2,2]) / torch.sum(test_cm)\n",
    "\n",
    "            # Saving some results\n",
    "            sub_training_losses.append(train_loss)\n",
    "            sub_validation_losses.append(val_loss)\n",
    "            sub_training_accuracies.append(training_accuracy)\n",
    "            sub_validation_accuracies.append(validation_accuracy)\n",
    "            sub_test_loss.append(test_loss)\n",
    "            sub_test_accuracy.append(test_accuracy)\n",
    "\n",
    "            print(\"Post-training Results:\")\n",
    "            print(f\"Train Loss: {train_loss[-1]:.4f}; Validation Loss: {val_loss[-1]:.4f}; Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Training Accuracy: {training_accuracy:.4f}; Test Accuracy: {test_accuracy:.4f}\"); print()\n",
    "            print(f\"Train Confusion Matrix:\\n{train_cm}\"); print()\n",
    "            print(f\"Test Confusion Matrix:\\n{test_cm}\"); print()\n",
    "\n",
    "            print(\"---------------------------------------------\")\n",
    "\n",
    "        # Saving some results\n",
    "        training_losses.append(sub_training_losses)\n",
    "        validation_losses.append(sub_validation_losses)\n",
    "        training_accuracies.append(sub_training_accuracies)\n",
    "        validation_accuracies.append(sub_validation_accuracies)\n",
    "        test_losses.append(sub_test_loss)\n",
    "        test_accuracies.append(sub_test_accuracy)\n",
    "\n",
    "    # results dictionary\n",
    "    results = {\n",
    "        'training_losses': training_losses,\n",
    "        'validation_losses': validation_losses,\n",
    "        'training_accuracies': training_accuracies,\n",
    "        'validation_accuracies': validation_accuracies,\n",
    "        'test_losses': test_losses,\n",
    "        'test_accuracies': test_accuracies\n",
    "    }\n",
    "\n",
    "    print(\"Experiments Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['training_losses', 'validation_losses', 'training_accuracies', 'validation_accuracies', 'test_losses', 'test_accuracies'])\n",
      "dict_keys(['[5]', '[10]', '[20]'])\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\envs\\act_smile\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "converted_results = convert_all_results(params, results)\n",
    "print(converted_results.keys())\n",
    "print(converted_results['training_losses'].keys())\n",
    "\n",
    "sys.exit()\n",
    "# TODO: Modify results storage to save results in a dictionary with a unique key for each combination of hyperparameters\n",
    "# TODO: Finish updating the code to use the new functions, create experiment analysis method and identify best model, add plots, create README.md, and push to GitHub \n",
    "# TODO: later, update ConfusionMatrix (and any other functions) to work with K classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING EXPERIMENTS\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer Architecture: [5]\n",
      "Epoch: 0; before training loss: 1.1604379415512085\n",
      "Epoch: 26749; after train loss: 7.276311953319237e-05\n",
      "\n",
      "Test loss post training 0.002410204615443945\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer Architecture: [10]\n",
      "Epoch: 0; before training loss: 1.1342799663543701\n",
      "Epoch: 49331; after train loss: 2.768605190794915e-05\n",
      "\n",
      "Test loss post training 0.0075431871227920055\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer Architecture: [20]\n",
      "Epoch: 0; before training loss: 1.1104189157485962\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 56\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# #printing epoch and loss \u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# if epoch % 5000 == 0:\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m#     print('Epoch: {} train loss: {}'.format(epoch,loss.item()))\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#backpropagation\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     59\u001b[0m training_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\act_smile\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\anaconda3\\envs\\act_smile\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if RUN_EXPERIMENTS:\n",
    "    print(\"RUNNING EXPERIMENTS\")\n",
    "    \n",
    "    all_training_loss = []\n",
    "    train_cm_storage = []\n",
    "    test_cm_storage = []\n",
    "    for i in params['learning_rate']:\n",
    "        training_loss = []\n",
    "        train_cm = []\n",
    "        test_cm = []\n",
    "        for size in params['hidden_size']:\n",
    "\n",
    "            #creating model and stopping criteria\n",
    "            model = Network(NUM_INPUTS, NUM_OUTPUTS, NUM_LAYERS, size)\n",
    "            stopping_criteria = StopCriteria()\n",
    "\n",
    "            #sending model to cuda\n",
    "            model.to(device)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss() #cross-entropy loss\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr = i) # implementing momentum for learning rate\n",
    "\n",
    "            #Showing test set loss pre-training\n",
    "            print(\"-----------------------------------------------------------------\")\n",
    "            print(f\"Learning Rate: {i}; Hidden Layer Architecture: {size}\")\n",
    "            \n",
    "            # Training model\n",
    "            for epoch in range(EPOCHS):\n",
    "                optimizer.zero_grad()\n",
    "                output = model.forward(X_train).squeeze()\n",
    "                loss = criterion(output, torch.max(y_train_encoded, 1)[1]) \n",
    "                \n",
    "                if epoch == 0:\n",
    "                    print('Epoch: {}; before training loss: {}'.format(epoch,loss.item()))\n",
    "\n",
    "                #implementing stopping criteria\n",
    "                val_output = model.forward(X_val)\n",
    "                val_loss = criterion(val_output.squeeze(), torch.max(y_val_encoded, 1)[1])\n",
    "\n",
    "                if stopping_criteria.step(val_loss):\n",
    "                    print('Epoch: {}; after train loss: {}'.format(epoch,loss.item()))\n",
    "                    print()\n",
    "                    break\n",
    "\n",
    "                # #printing epoch and loss \n",
    "                # if epoch % 5000 == 0 and epoch != 0:\n",
    "                #     print('Epoch: {} train loss: {}'.format(epoch,loss.item()))\n",
    "\n",
    "                #backpropagation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            training_loss.append(loss.item())\n",
    "            # evaluating the model and storing relevant information\n",
    "            \n",
    "            \n",
    "            model.eval()\n",
    "            train_pred = model(X_train)\n",
    "            test_pred = model(X_test)\n",
    "            train_CM = ConfusionMatrix(model, train_pred, y_train_encoded)\n",
    "            test_CM = ConfusionMatrix(model, test_pred, y_test_encoded)\n",
    "            after_train = criterion(test_pred.squeeze(), torch.max(y_test_encoded, 1)[1])\n",
    "            \n",
    "            print('Test loss post training: ' , after_train.item())\n",
    "            print()\n",
    "            print(\"Training Confusion Matrix: \\n\" + str(train_CM))\n",
    "            print()\n",
    "            print(\"Test Confusion Matrix: \\n\" + str(test_CM))\n",
    "            print()\n",
    "            print(\"-----------------------------------------------------------------\")\n",
    "            \n",
    "            train_cm.append(train_CM)\n",
    "            test_cm.append(test_CM)\n",
    "\n",
    "        all_training_loss.append(training_loss)\n",
    "        train_cm_storage.append(train_cm)\n",
    "        test_cm_storage.append(test_cm)\n",
    "    \n",
    "    all_training_loss = np.array(all_training_loss)\n",
    "    train_cm_storage = np.array(train_cm_storage)\n",
    "    test_cm_storage = np.array(test_cm_storage)\n",
    "    print()\n",
    "    print(\"DONE TESTING HYPERPARAMETERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr_index, best_hs_index = np.argmin(all_training_loss).tolist()\n",
    "best_lr = params['learning_rate'][best_lr_index]\n",
    "best_hs = params['hidden_size'][best_hs_index]\n",
    "\n",
    "print(\"Best Learning Rate: \" + str(best_lr))    \n",
    "print(\"Best Hidden Layer PE: \" + str(best_hs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BEST_MODEL:\n",
    "    print(\"RUNNING BEST MODEL\")\n",
    "\n",
    "    #Initializing model and stopping criteria classes\n",
    "    model = Network(best_hs)\n",
    "    stopping_criteria = StopCriteria()\n",
    "\n",
    "    #sending model to cuda\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss() #cross-entropy loss\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = best_lr) # implementing momentum for learning rate\n",
    "\n",
    "    # Training model\n",
    "    for epoch in range(EPOCHS):\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(X_train)\n",
    "        loss = criterion(output.squeeze(), torch.max(y_train_encoded, 1)[1]) \n",
    "\n",
    "        #implementing stopping criteria\n",
    "        val_output = model.forward(X_val)\n",
    "        val_loss = criterion(val_output.squeeze(), torch.max(y_val_encoded, 1)[1])\n",
    "\n",
    "        if stopping_criteria.step(val_loss):\n",
    "            print('Epoch: {}; after train loss: {}'.format(epoch,loss.item()))\n",
    "            break\n",
    "\n",
    "        #printing epoch and loss \n",
    "        if epoch % 250 == 0:\n",
    "            print('Epoch: {}; train loss: {}'.format(epoch,loss.item()))\n",
    "\n",
    "        #backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after Training 0.0060800062492489815\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = model(X_test).squeeze()\n",
    "after_train = criterion(y_pred, torch.max(y_test_encoded, 1)[1]) \n",
    "print('Test loss after Training' , after_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Confusion Matrix:\n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n"
     ]
    }
   ],
   "source": [
    "test_CM = ConfusionMatrix(model, y_pred, y_test_encoded)\n",
    "print(\"Test Confusion Matrix:\")\n",
    "print(test_CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Model Discussion:\n",
    "\n",
    "The overall best model in terms of test set loss after training is the model with a learning rate of 1 and 5 hidden PE. This model achieved a testing loss of 0.00117. This learning rate may be performing the best because it can quickly get over non-optimal local minima on the performance surface before the training is terminated by the stopping criteria. It may be possible to achieve lower train and test loss with more patience on the stopping criteria for lower learning rates - however this would increase the number of epochs required for convergence and increase computation time. Five PE in the hidden layer may be performing well because not all of the inputs are predictive of the wine cultivar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 5), (5, 10), (10, 5), (10, 10)]\n"
     ]
    }
   ],
   "source": [
    "# LOCAL PARAMETERS\n",
    "NUM_LAYERS = 2\n",
    "NUM_INPUTS = X_train.shape[1]\n",
    "NUM_OUTPUTS = y_train_encoded.shape[1]\n",
    "pairs = list(itertools.product([5, 10], repeat=2))\n",
    "print(pairs)\n",
    "\n",
    "params = {\n",
    "    'learning_rate': [1.0, 0.1, 0.01, 0.001],\n",
    "    'hidden_size': pairs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING EXPERIMENTS\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer Architecture: (5, 5)\n",
      "Epoch: 0; before training loss: 1.1175674200057983\n",
      "Epoch: 6; after train loss: 1.085118293762207\n",
      "\n",
      "Test loss post training 1.124037265777588\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[ 0,  0,  0],\n",
      "        [45, 58, 39],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[ 0,  0,  0],\n",
      "        [11,  8,  9],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer Architecture: (5, 10)\n",
      "Epoch: 0; before training loss: 1.1287072896957397\n",
      "Epoch: 45010; after train loss: 2.2319885829347186e-05\n",
      "\n",
      "Test loss post training 0.0008015120984055102\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer Architecture: (10, 5)\n",
      "Epoch: 0; before training loss: 1.1555646657943726\n",
      "Epoch: 16127; after train loss: 9.019799472298473e-05\n",
      "\n",
      "Test loss post training 0.00028961998759768903\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer Architecture: (10, 10)\n",
      "Epoch: 0; before training loss: 1.088831901550293\n",
      "Epoch: 5; after train loss: 1.0766799449920654\n",
      "\n",
      "Test loss post training 1.113846778869629\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[ 0,  0,  0],\n",
      "        [45, 58, 39],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[ 0,  0,  0],\n",
      "        [11,  8,  9],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.1; Hidden Layer Architecture: (5, 5)\n",
      "Epoch: 0; before training loss: 1.0888515710830688\n",
      "Epoch: 45896; after train loss: 0.0004990726010873914\n",
      "\n",
      "Test loss post training 0.0007844573701731861\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.1; Hidden Layer Architecture: (5, 10)\n",
      "Epoch: 0; before training loss: 1.1878950595855713\n",
      "Epoch: 23003; after train loss: 0.00058378727408126\n",
      "\n",
      "Test loss post training 0.000940091791562736\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.1; Hidden Layer Architecture: (10, 5)\n",
      "Epoch: 0; before training loss: 1.298940896987915\n",
      "Epoch: 24; after train loss: 1.0914949178695679\n",
      "\n",
      "Test loss post training 1.1163413524627686\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[ 0,  0,  0],\n",
      "        [45, 58, 39],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[ 0,  0,  0],\n",
      "        [11,  8,  9],\n",
      "        [ 0,  0,  0]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.1; Hidden Layer Architecture: (10, 10)\n",
      "Epoch: 0; before training loss: 1.211868405342102\n",
      "Epoch: 89694; after train loss: 0.00010904189548455179\n",
      "\n",
      "Test loss post training 0.0007938466151244938\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "DONE TESTING HYPERPARAMETERS\n"
     ]
    }
   ],
   "source": [
    "if RUN_EXPERIMENTS:\n",
    "    print(\"RUNNING EXPERIMENTS\")\n",
    "    \n",
    "    for i in params['learning_rate']:\n",
    "        for size in params['hidden_size']:\n",
    "            \n",
    "            model2 = Network(NUM_INPUTS, NUM_OUTPUTS, NUM_LAYERS, size)\n",
    "            stopping_criteria = StopCriteria()\n",
    "\n",
    "            #sending model to cuda\n",
    "            model2.to(device)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss() #cross-entropy loss\n",
    "            optimizer = torch.optim.SGD(model2.parameters(), lr = i) # implementing momentum for learning rate\n",
    "\n",
    "            #Showing test set loss pre-training\n",
    "            print(\"-----------------------------------------------------------------\")\n",
    "            print(f\"Learning Rate: {i}; Hidden Layer Architecture: {size}\")\n",
    "\n",
    "            # Training model\n",
    "            for epoch in range(EPOCHS):\n",
    "                optimizer.zero_grad()\n",
    "                output = model2.forward(X_train).squeeze()\n",
    "                loss = criterion(output, torch.max(y_train_encoded, 1)[1]) \n",
    "\n",
    "                if epoch == 0:\n",
    "                    print('Epoch: {}; before training loss: {}'.format(epoch,loss.item()))\n",
    "\n",
    "                #implementing stopping criteria\n",
    "                val_output = model2.forward(X_val)\n",
    "                val_loss = criterion(val_output.squeeze(), torch.max(y_val_encoded, 1)[1])\n",
    "\n",
    "                if stopping_criteria.step(val_loss):\n",
    "                    print('Epoch: {}; after train loss: {}'.format(epoch,loss.item()))\n",
    "                    print()\n",
    "                    break\n",
    "\n",
    "                #backpropagation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # evaluating the model and storing relevant information\n",
    "            model2.eval()\n",
    "            train_pred = model2(X_train)\n",
    "            test_pred = model2(X_test)\n",
    "            train_CM = ConfusionMatrix(model2, train_pred, y_train_encoded)\n",
    "            test_CM = ConfusionMatrix(model2, test_pred, y_test_encoded)\n",
    "            after_train = criterion(test_pred.squeeze(), torch.max(y_test_encoded, 1)[1])\n",
    "\n",
    "            print('Test loss post training' , after_train.item())\n",
    "            print()\n",
    "            print(\"Training Confusion Matrix: \\n\" + str(train_CM))\n",
    "            print()\n",
    "            print(\"Test Confusion Matrix: \\n\" + str(test_CM))\n",
    "            print()\n",
    "            print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "    print()\n",
    "    print(\"DONE TESTING HYPERPARAMETERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model with no hidden-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOCAL PARAMETERS\n",
    "NUM_LAYERS = 0 # no hidden layers\n",
    "SIZE = [] # no hidden layers\n",
    "NUM_INPUTS = X_train.shape[1]\n",
    "NUM_OUTPUTS = y_train_encoded.shape[1]\n",
    "\n",
    "params = {\n",
    "    'learning_rate': [1.0, 0.1, 0.01, 0.001]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING EXPERIMENTS\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 1.0; Hidden Layer Architecture: []\n",
      "Epoch: 0; before training loss: 1.4978491067886353\n",
      "Epoch: 38384; after train loss: 0.00011088434257544577\n",
      "\n",
      "Test loss post training 0.014064214192330837\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.1; Hidden Layer Architecture: []\n",
      "Epoch: 0; before training loss: 1.202933430671692\n",
      "Epoch: 79037; after train loss: 0.0005150689394213259\n",
      "\n",
      "Test loss post training 0.011468115262687206\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.01; Hidden Layer Architecture: []\n",
      "Epoch: 0; before training loss: 1.2941336631774902\n",
      "Epoch: 162327; after train loss: 0.0023208828642964363\n",
      "\n",
      "Test loss post training 0.009471285156905651\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "Learning Rate: 0.001; Hidden Layer Architecture: []\n",
      "Epoch: 0; before training loss: 1.244872808456421\n",
      "Epoch: 318724; after train loss: 0.009728962555527687\n",
      "\n",
      "Test loss post training 0.010644534602761269\n",
      "\n",
      "Training Confusion Matrix: \n",
      "tensor([[45,  0,  0],\n",
      "        [ 0, 58,  0],\n",
      "        [ 0,  0, 39]])\n",
      "\n",
      "Test Confusion Matrix: \n",
      "tensor([[11,  0,  0],\n",
      "        [ 0,  8,  0],\n",
      "        [ 0,  0,  9]])\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "DONE TESTING HYPERPARAMETERS\n"
     ]
    }
   ],
   "source": [
    "if RUN_EXPERIMENTS:\n",
    "    print(\"RUNNING EXPERIMENTS\")\n",
    "    \n",
    "    for i in params['learning_rate']:\n",
    "        model3 = Network(NUM_INPUTS, NUM_OUTPUTS, NUM_LAYERS, SIZE)\n",
    "        stopping_criteria = StopCriteria(patience = 10)\n",
    "\n",
    "        #sending model to cuda\n",
    "        model3.to(device)\n",
    "\n",
    "        # initializing criterion and optimizer\n",
    "        criterion = nn.CrossEntropyLoss() #cross-entropy loss\n",
    "        optimizer = torch.optim.SGD(model3.parameters(), lr = i) # implementing momentum for learning rate\n",
    "\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(f\"Learning Rate: {i}; Hidden Layer Architecture: {SIZE}\")\n",
    "\n",
    "        # Training model\n",
    "        for epoch in range(EPOCHS):\n",
    "            optimizer.zero_grad()\n",
    "            output = model3.forward(X_train).squeeze()\n",
    "            loss = criterion(output, torch.max(y_train_encoded, 1)[1]) \n",
    "\n",
    "            if epoch == 0:\n",
    "                print('Epoch: {}; before training loss: {}'.format(epoch,loss.item()))\n",
    "\n",
    "            #implementing stopping criteria\n",
    "            val_output = model3.forward(X_val)\n",
    "            val_loss = criterion(val_output.squeeze(), torch.max(y_val_encoded, 1)[1])\n",
    "\n",
    "            if stopping_criteria.step(val_loss):\n",
    "                print('Epoch: {}; after train loss: {}'.format(epoch,loss.item()))\n",
    "                print()\n",
    "                break\n",
    "\n",
    "            # #printing epoch and loss \n",
    "            # if epoch % 5000 == 0 and epoch != 0:\n",
    "            #     print('Epoch: {} train loss: {}'.format(epoch,loss.item()))\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluating the model and storing relevant information\n",
    "        model3.eval()\n",
    "        train_pred = model3(X_train)\n",
    "        test_pred = model3(X_test)\n",
    "        train_CM = ConfusionMatrix(model3, train_pred, y_train_encoded)\n",
    "        test_CM = ConfusionMatrix(model3, test_pred, y_test_encoded)\n",
    "        after_train = criterion(test_pred.squeeze(), torch.max(y_test_encoded, 1)[1])\n",
    "\n",
    "        print('Test loss post training' , after_train.item())\n",
    "        print()\n",
    "        print(\"Training Confusion Matrix: \\n\" + str(train_CM))\n",
    "        print()\n",
    "        print(\"Test Confusion Matrix: \\n\" + str(test_CM))\n",
    "        print()\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "    \n",
    "print()\n",
    "print(\"DONE TESTING HYPERPARAMETERS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of no hidden layer model:\n",
    "\n",
    "A model with no hidden layers is unable to classify the Wine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
